{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import find_dotenv, load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_type = os.getenv(\"api_type\")\n",
        "openai.api_base = os.getenv(\"api_base\")\n",
        "openai.api_version = os.getenv(\"api_version\")\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "# RCI Chain with ChatModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T4xsy4ND2av"
      },
      "source": [
        "## Multi Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ezZ5D55uD-Qm"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "P_Vz09usvqhb"
      },
      "outputs": [],
      "source": [
        "model = AzureChatOpenAI(\n",
        "        deployment_name=\"chatgpt-gpt35-turbo\",\n",
        "        model_name=\"gpt-35-turbo\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=1000\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Z6HVNGkvv9-G"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"tell me an intersting fact about {subject}\"\n",
        "    )\n",
        "\n",
        "reverse_prompt = ChatPromptTemplate.from_template(\n",
        "    \"based on this interesting fact which is chunked down from a meta subject:\\n\\n {interesting_fact}\\n\\n Recover what the meta subject is\\n Subject:\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "UoeILxMtwS-A"
      },
      "outputs": [],
      "source": [
        "chain = prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "cm8y8Ll4wJMH",
        "outputId": "8f647e6e-158d-4eeb-b3f1-793191c07e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"Laptop\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"Laptop\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"tell me an intersting fact about Laptop\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: tell me an intersting fact about Laptop\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:AzureChatOpenAI] [2.33s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The first laptop computer, the Grid Compass, was developed in 1982 and weighed 11 pounds, which was considered lightweight at the time.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The first laptop computer, the Grid Compass, was developed in 1982 and weighed 11 pounds, which was considered lightweight at the time.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 29,\n",
            "      \"prompt_tokens\": 17,\n",
            "      \"total_tokens\": 46\n",
            "    },\n",
            "    \"model_name\": \"gpt-35-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The first laptop computer, the Grid Compass, was developed in 1982 and weighed 11 pounds, which was considered lightweight at the time.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [2.33s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The first laptop computer, the Grid Compass, was developed in 1982 and weighed 11 pounds, which was considered lightweight at the time.\"\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The first laptop computer, the Grid Compass, was developed in 1982 and weighed 11 pounds, which was considered lightweight at the time.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"subject\": \"Laptop\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jRzytVdiEwX0"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "\n",
        "langchain.debug = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "K82XYV2zGnWg"
      },
      "outputs": [],
      "source": [
        "chain1 = prompt | model | StrOutputParser()\n",
        "\n",
        "chain2 = {\"interesting_fact\": chain1} | reverse_prompt | model | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSVepkyWG0rV",
        "outputId": "dc71faa6-79f9-4ce0-c621-9975dafa796d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"Laptop\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"subject\": \"Laptop\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"Laptop\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"subject\": \"Laptop\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"tell me an intersting fact about Laptop\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 5:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: tell me an intersting fact about Laptop\"\n",
            "  ]\n",
            "}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 5:llm:AzureChatOpenAI] [2.70s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 54,\n",
            "      \"prompt_tokens\": 17,\n",
            "      \"total_tokens\": 71\n",
            "    },\n",
            "    \"model_name\": \"gpt-35-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 6:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence > 6:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableSequence] [2.70s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap] [2.71s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"interesting_fact\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"interesting_fact\": \"The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"based on this interesting fact which is chunked down from a meta subject:\\n\\n The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\\n\\n Recover what the meta subject is\\n Subject:\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: based on this interesting fact which is chunked down from a meta subject:\\n\\n The first laptop ever made was the Epson HX-20 in 1981, which was also the first computer to be called a \\\"laptop.\\\" It had a 4-line LCD screen and could run for up to 20 hours on four AA batteries.\\n\\n Recover what the meta subject is\\n Subject:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:AzureChatOpenAI] [426ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"History of Laptops\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"History of Laptops\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 4,\n",
            "      \"prompt_tokens\": 86,\n",
            "      \"total_tokens\": 90\n",
            "    },\n",
            "    \"model_name\": \"gpt-35-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"History of Laptops\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.13s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"History of Laptops\"\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'History of Laptops'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain2.invoke({\"subject\": \"Laptop\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqDy4FroigDu"
      },
      "source": [
        "# Testing RCI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "cUOH6rVBiNOP"
      },
      "outputs": [],
      "source": [
        "langchain.debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "8PrgOsByPHZT"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Nb1gtx1vhopk"
      },
      "outputs": [],
      "source": [
        "template=\"You are a helpful assistant that imparts wisdom and guides people with accurate answers.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{question}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Z1vbc_tShv3-"
      },
      "outputs": [],
      "source": [
        "chain1 = chat_prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TQxR1btKhv4J"
      },
      "outputs": [],
      "source": [
        "initial_question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Qnczl4rWhv4J",
        "outputId": "6561eb6f-f1f0-417e-e549-8c8cc97189c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Roger has a total of 11 tennis balls now. \\n\\nHere's how you can break it down: \\n\\n- Roger starts with 5 tennis balls. \\n- He buys 2 cans of tennis balls, so he adds 2 x 3 = 6 tennis balls to his collection. \\n- In total, Roger now has 5 + 6 = 11 tennis balls.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_answer = chain1.invoke({\"question\": initial_question})\n",
        "initial_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "dt4wOS1wPDDG"
      },
      "outputs": [],
      "source": [
        "fake_initial_ai_answer = \"\"\"Roger initially has 5 tennis balls. Each can of tennis balls contains 3 tennis balls, and he bought 2 cans, so he has 2 x 3 = 6 additional tennis balls.\n",
        "Therefore, the total number of tennis balls Roger has now is 5 + 4 = 9.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdifZhmeh5fK"
      },
      "source": [
        "## Part 2 - Critique  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Lyn5cvz2gS2l"
      },
      "outputs": [],
      "source": [
        "template=\"You are a helpful assistant that looks at answers and finds what is wrong with them based on the original question given.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"### Question:\\n\\n{question}\\n\\n ###Answer Given:{initial_answer}\\n\\n Review your previous answer and find problems with your answer\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8Ud512H8gS2x"
      },
      "outputs": [],
      "source": [
        "rc_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VSz9eWU6gagJ"
      },
      "outputs": [],
      "source": [
        "chain2 = rc_prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xyrzdgNbg-u6",
        "outputId": "08055a11-4e0b-4806-f704-8dd451fbe0b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The calculation for the additional tennis balls is incorrect. Two cans of tennis balls with 3 tennis balls each would result in 2 x 3 x 3 = 18 additional tennis balls, not 6. Therefore, the total number of tennis balls Roger has now is 5 + 18 = 23, not 9.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "constructive_criticism = chain2.invoke({\"question\": initial_question, \"initial_answer\":fake_initial_ai_answer})\n",
        "constructive_criticism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThJCqWYXi5To"
      },
      "source": [
        "## Part 3 - The Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "GF9xkoAJjIZr"
      },
      "outputs": [],
      "source": [
        "template=\"You are a helpful assistant that reviews answers and critiques based on the original question given and write a new improved final answer.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"### Question:\\n\\n{question}\\n\\n ###Answer Given:{initial_answer}\\n\\n \\\n",
        "###Constructive Criticism:{constructive_criticism}\\n\\n Based on the problems you found, improve your answer.\\n\\n### Final Answer:\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kUVYEp0pjIZ5"
      },
      "outputs": [],
      "source": [
        "improvement_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "IUGNqQg_l0st"
      },
      "outputs": [],
      "source": [
        "chain3 = improvement_prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yUkfpDedl975",
        "outputId": "2c277e29-9d68-4820-d4b4-c87ec3db927d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Roger initially has 5 tennis balls. He bought 2 cans of tennis balls, and each can has 3 tennis balls, so he has 2 x 3 = 6 additional tennis balls. Therefore, the total number of tennis balls Roger has now is 5 + 6 + 9.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_result = chain3.invoke({\"question\": initial_question,\n",
        "                              \"initial_answer\":fake_initial_ai_answer,\n",
        "                              \"constructive_criticism\": constructive_criticism})\n",
        "\n",
        "final_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQPHAp5emT9K"
      },
      "source": [
        "## Combined Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "v7coUHRTrXW5"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "QhXNxN1ZmLqL"
      },
      "outputs": [],
      "source": [
        "\n",
        "chain1 = chat_prompt | model | StrOutputParser()\n",
        "\n",
        "critque_chain = {\"question\": itemgetter(\"question\"),\n",
        "                 \"initial_answer\": chain1 } | rc_prompt | model | StrOutputParser()\n",
        "\n",
        "chain3 = {\"question\": itemgetter(\"question\"),\n",
        "          \"initial_answer\": chain1,\n",
        "          \"constructive_criticism\": critque_chain} | improvement_prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "JvTj9xb0rLWG",
        "outputId": "806c63ff-69de-4325-d8fc-5167c8ef67ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'A better SMS message to say you are tired could be \"I\\'m tired\". This response is short, simple, and to the point, which is appropriate for an SMS message format.'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain3.invoke({\"question\":\"Write an sms message to say I am tired\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ihFvgx2ksWtG"
      },
      "outputs": [],
      "source": [
        "langchain.debug = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z_FiZIqBstKY",
        "outputId": "97124c46-5fb5-43dd-a1d0-f80ec9aacf6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"question\": \"Write an sms message to say I am tired\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 3:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"SystemMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"You are a helpful assistant that imparts wisdom and guides people with accurate answers.\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"Write an sms message to say I am tired\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 6:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful assistant that imparts wisdom and guides people with accurate answers.\\nHuman: Write an sms message to say I am tired\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": {\n",
            "    \"question\": \"Write an sms message to say I am tired\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 7:chain:RunnableLambda] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 7:chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 9:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 9:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"SystemMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"You are a helpful assistant that imparts wisdom and guides people with accurate answers.\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"Write an sms message to say I am tired\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 10:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful assistant that imparts wisdom and guides people with accurate answers.\\nHuman: Write an sms message to say I am tired\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 10:llm:AzureChatOpenAI] [2.23s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 14,\n",
            "      \"prompt_tokens\": 38,\n",
            "      \"total_tokens\": 52\n",
            "    },\n",
            "    \"model_name\": \"gpt-35-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 11:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence > 11:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap > 8:chain:RunnableSequence] [2.24s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 6:chain:RunnableMap] [2.24s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\",\n",
            "  \"initial_answer\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 12:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Write an sms message to say I am tired\",\n",
            "  \"initial_answer\": \"\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 12:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"chat\",\n",
            "    \"ChatPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"messages\": [\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"SystemMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"You are a helpful assistant that looks at answers and finds what is wrong with them based on the original question given.\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"lc\": 1,\n",
            "        \"type\": \"constructor\",\n",
            "        \"id\": [\n",
            "          \"langchain\",\n",
            "          \"schema\",\n",
            "          \"messages\",\n",
            "          \"HumanMessage\"\n",
            "        ],\n",
            "        \"kwargs\": {\n",
            "          \"content\": \"### Question:\\n\\nWrite an sms message to say I am tired\\n\\n ###Answer Given:\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\\n\\n Review your previous answer and find problems with your answer\",\n",
            "          \"additional_kwargs\": {}\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 5:chain:RunnableSequence > 13:llm:AzureChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You are a helpful assistant that looks at answers and finds what is wrong with them based on the original question given.\\nHuman: ### Question:\\n\\nWrite an sms message to say I am tired\\n\\n ###Answer Given:\\\"I'm feeling exhausted and need to rest. Talk to you later.\\\"\\n\\n Review your previous answer and find problems with your answer\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 6:llm:AzureChatOpenAI] [2.51s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"\\\"I'm feeling exhausted and in need of some rest. Going to take a break now. Talk to you later.\\\"\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"\\\"I'm feeling exhausted and in need of some rest. Going to take a break now. Talk to you later.\\\"\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 23,\n",
            "      \"prompt_tokens\": 38,\n",
            "      \"total_tokens\": 61\n",
            "    },\n",
            "    \"model_name\": \"gpt-35-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 7:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence > 7:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\\"I'm feeling exhausted and in need of some rest. Going to take a break now. Talk to you later.\\\"\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableMap > 4:chain:RunnableSequence] [2.52s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"\\\"I'm feeling exhausted and in need of some rest. Going to take a break now. Talk to you later.\\\"\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "chain3.invoke({\"question\":\"Write an sms message to say I am tired\"})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
